{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " 01 - Introduction to Deep Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4bggkaPuBo0rPvFAFF4I+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniloaleixo/deep-rl/blob/master/01_Introduction_to_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw0_fjVUlZAe",
        "colab_type": "text"
      },
      "source": [
        "# Introduction do Deep Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBgQxyMjp07P",
        "colab_type": "text"
      },
      "source": [
        "From tutorial: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJAeJ_tild49",
        "colab_type": "text"
      },
      "source": [
        "## Install dependencies for the CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj28_8zwlQVg",
        "colab_type": "code",
        "outputId": "f2564459-345f-44a3-82b7-d8e377ffd496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install h5py\n",
        "!pip install keras-rl\n",
        "!pip install gym\n",
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.17.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.10)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting tensorflow==1.14\n",
            "  Downloading tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2 MB 5.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.1.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.27.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (45.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.1.0\n",
            "    Uninstalling tensorflow-estimator-2.1.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.1.1\n",
            "    Uninstalling tensorboard-2.1.1:\n",
            "      Successfully uninstalled tensorboard-2.1.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.1.0\n",
            "    Uninstalling tensorflow-2.1.0:\n",
            "      Successfully uninstalled tensorflow-2.1.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X34f8PcImXVv",
        "colab_type": "text"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AqrSUnFlVnu",
        "colab_type": "code",
        "outputId": "e1325c3b-c2cc-4e01-8996-5ebdc1e51209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwoRwrZMmnMa",
        "colab_type": "text"
      },
      "source": [
        "Then set the relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNlcFddulbWw",
        "colab_type": "code",
        "outputId": "349e7e25-b033-4dad-9c53-3fc9c9b322a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ENV_NAME = 'CartPole-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions available in the Cartpole problem\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oooshh6omsGU",
        "colab_type": "text"
      },
      "source": [
        "Next, we will build a very simple single hidden layer neural network model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAQRj06SmpKY",
        "colab_type": "code",
        "outputId": "b637e209-da20-4c8d-a503-3f4833996555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 34        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cqtOms7mvyN",
        "colab_type": "text"
      },
      "source": [
        "Now, configure and compile our agent. We will set our policy as Epsilon Greedy and our memory as Sequential Memory because we want to store the result of actions we performed and the rewards we get for each action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVPM7_eymth_",
        "colab_type": "code",
        "outputId": "26b3c23e-b362-4942-d2bf-30ff43e98578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "policy = EpsGreedyQPolicy()\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
        "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 5000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   48/5000: episode: 1, duration: 0.589s, episode steps: 48, steps per second: 81, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.168 [-0.239, 0.766], loss: 0.450238, mean_absolute_error: 0.522733, mean_q: -0.001673\n",
            "  104/5000: episode: 2, duration: 0.160s, episode steps: 56, steps per second: 349, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.150, 1.128], loss: 0.369756, mean_absolute_error: 0.456440, mean_q: 0.127125\n",
            "  142/5000: episode: 3, duration: 0.111s, episode steps: 38, steps per second: 342, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.093 [-0.226, 0.714], loss: 0.296629, mean_absolute_error: 0.452627, mean_q: 0.308771\n",
            "  181/5000: episode: 4, duration: 0.117s, episode steps: 39, steps per second: 332, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.069 [-0.546, 0.798], loss: 0.249698, mean_absolute_error: 0.485811, mean_q: 0.486384\n",
            "  216/5000: episode: 5, duration: 0.103s, episode steps: 35, steps per second: 338, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.091 [-0.396, 0.886], loss: 0.203095, mean_absolute_error: 0.527339, mean_q: 0.684187\n",
            "  245/5000: episode: 6, duration: 0.083s, episode steps: 29, steps per second: 350, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.087 [-0.349, 0.824], loss: 0.170173, mean_absolute_error: 0.585451, mean_q: 0.879189\n",
            "  270/5000: episode: 7, duration: 0.074s, episode steps: 25, steps per second: 339, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.091 [-0.397, 0.859], loss: 0.146549, mean_absolute_error: 0.653434, mean_q: 1.062687\n",
            "  292/5000: episode: 8, duration: 0.067s, episode steps: 22, steps per second: 330, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.353, 0.912], loss: 0.125347, mean_absolute_error: 0.717813, mean_q: 1.238206\n",
            "  310/5000: episode: 9, duration: 0.056s, episode steps: 18, steps per second: 321, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.427, 1.096], loss: 0.111342, mean_absolute_error: 0.785343, mean_q: 1.407140\n",
            "  333/5000: episode: 10, duration: 0.074s, episode steps: 23, steps per second: 313, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.081 [-0.428, 0.901], loss: 0.096333, mean_absolute_error: 0.853701, mean_q: 1.571234\n",
            "  355/5000: episode: 11, duration: 0.063s, episode steps: 22, steps per second: 350, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.365, 0.933], loss: 0.083187, mean_absolute_error: 0.926685, mean_q: 1.755943\n",
            "  375/5000: episode: 12, duration: 0.061s, episode steps: 20, steps per second: 330, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.096 [-0.562, 1.267], loss: 0.077967, mean_absolute_error: 1.001933, mean_q: 1.946295\n",
            "  395/5000: episode: 13, duration: 0.059s, episode steps: 20, steps per second: 341, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.089 [-0.572, 1.099], loss: 0.081763, mean_absolute_error: 1.096330, mean_q: 2.107790\n",
            "  410/5000: episode: 14, duration: 0.044s, episode steps: 15, steps per second: 339, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.093 [-0.790, 1.243], loss: 0.095181, mean_absolute_error: 1.161689, mean_q: 2.257880\n",
            "  422/5000: episode: 15, duration: 0.036s, episode steps: 12, steps per second: 332, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.110 [-0.776, 1.266], loss: 0.098237, mean_absolute_error: 1.217393, mean_q: 2.369110\n",
            "  439/5000: episode: 16, duration: 0.051s, episode steps: 17, steps per second: 331, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.073 [-0.763, 1.224], loss: 0.110304, mean_absolute_error: 1.273516, mean_q: 2.490260\n",
            "  456/5000: episode: 17, duration: 0.055s, episode steps: 17, steps per second: 307, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.087 [-0.794, 1.254], loss: 0.120020, mean_absolute_error: 1.362563, mean_q: 2.647918\n",
            "  470/5000: episode: 18, duration: 0.044s, episode steps: 14, steps per second: 321, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.123 [-0.763, 1.329], loss: 0.142956, mean_absolute_error: 1.437748, mean_q: 2.790407\n",
            "  481/5000: episode: 19, duration: 0.036s, episode steps: 11, steps per second: 302, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-0.784, 1.384], loss: 0.161975, mean_absolute_error: 1.488647, mean_q: 2.875545\n",
            "  491/5000: episode: 20, duration: 0.031s, episode steps: 10, steps per second: 322, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.151 [-0.964, 1.727], loss: 0.166568, mean_absolute_error: 1.556827, mean_q: 2.978165\n",
            "  502/5000: episode: 21, duration: 0.037s, episode steps: 11, steps per second: 301, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.111 [-0.990, 1.556], loss: 0.236242, mean_absolute_error: 1.642398, mean_q: 3.119092\n",
            "  511/5000: episode: 22, duration: 0.027s, episode steps: 9, steps per second: 328, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.143 [-0.949, 1.739], loss: 0.236578, mean_absolute_error: 1.648618, mean_q: 3.163648\n",
            "  522/5000: episode: 23, duration: 0.037s, episode steps: 11, steps per second: 296, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.130 [-0.971, 1.728], loss: 0.209431, mean_absolute_error: 1.686068, mean_q: 3.258074\n",
            "  535/5000: episode: 24, duration: 0.039s, episode steps: 13, steps per second: 334, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.085 [-0.996, 1.587], loss: 0.304793, mean_absolute_error: 1.780120, mean_q: 3.379545\n",
            "  545/5000: episode: 25, duration: 0.031s, episode steps: 10, steps per second: 326, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.124 [-1.165, 2.058], loss: 0.174782, mean_absolute_error: 1.753403, mean_q: 3.432145\n",
            "  557/5000: episode: 26, duration: 0.036s, episode steps: 12, steps per second: 331, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.088 [-1.188, 1.918], loss: 0.177083, mean_absolute_error: 1.811932, mean_q: 3.555722\n",
            "  567/5000: episode: 27, duration: 0.031s, episode steps: 10, steps per second: 327, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.107 [-1.028, 1.611], loss: 0.257185, mean_absolute_error: 1.902331, mean_q: 3.712900\n",
            "  578/5000: episode: 28, duration: 0.033s, episode steps: 11, steps per second: 331, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.138 [-0.939, 1.699], loss: 0.226091, mean_absolute_error: 1.938479, mean_q: 3.803014\n",
            "  589/5000: episode: 29, duration: 0.044s, episode steps: 11, steps per second: 250, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-0.942, 1.734], loss: 0.183260, mean_absolute_error: 1.998953, mean_q: 3.915806\n",
            "  601/5000: episode: 30, duration: 0.040s, episode steps: 12, steps per second: 298, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.099 [-1.202, 1.744], loss: 0.325646, mean_absolute_error: 2.084411, mean_q: 4.053186\n",
            "  611/5000: episode: 31, duration: 0.030s, episode steps: 10, steps per second: 330, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.110 [-1.025, 1.615], loss: 0.404186, mean_absolute_error: 2.146519, mean_q: 4.119224\n",
            "  622/5000: episode: 32, duration: 0.033s, episode steps: 11, steps per second: 331, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.122 [-0.958, 1.592], loss: 0.312939, mean_absolute_error: 2.162149, mean_q: 4.131895\n",
            "  632/5000: episode: 33, duration: 0.030s, episode steps: 10, steps per second: 331, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.129 [-0.933, 1.664], loss: 0.391040, mean_absolute_error: 2.222637, mean_q: 4.252463\n",
            "  641/5000: episode: 34, duration: 0.027s, episode steps: 9, steps per second: 328, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.130 [-1.197, 1.894], loss: 0.589961, mean_absolute_error: 2.332420, mean_q: 4.310483\n",
            "  657/5000: episode: 35, duration: 0.049s, episode steps: 16, steps per second: 324, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.075 [-0.997, 1.467], loss: 0.425729, mean_absolute_error: 2.290796, mean_q: 4.343171\n",
            "  670/5000: episode: 36, duration: 0.041s, episode steps: 13, steps per second: 320, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.096 [-0.959, 1.441], loss: 0.484682, mean_absolute_error: 2.372312, mean_q: 4.474173\n",
            "  680/5000: episode: 37, duration: 0.030s, episode steps: 10, steps per second: 328, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.126 [-0.958, 1.535], loss: 0.413945, mean_absolute_error: 2.392001, mean_q: 4.600606\n",
            "  693/5000: episode: 38, duration: 0.040s, episode steps: 13, steps per second: 326, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.098 [-0.980, 1.481], loss: 0.471729, mean_absolute_error: 2.454801, mean_q: 4.725461\n",
            "  703/5000: episode: 39, duration: 0.030s, episode steps: 10, steps per second: 331, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.108 [-0.989, 1.486], loss: 0.617734, mean_absolute_error: 2.539230, mean_q: 4.788705\n",
            "  712/5000: episode: 40, duration: 0.028s, episode steps: 9, steps per second: 322, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.144 [-0.976, 1.579], loss: 0.405201, mean_absolute_error: 2.519630, mean_q: 4.832383\n",
            "  723/5000: episode: 41, duration: 0.038s, episode steps: 11, steps per second: 293, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-1.025, 1.496], loss: 0.506403, mean_absolute_error: 2.579652, mean_q: 4.948521\n",
            "  733/5000: episode: 42, duration: 0.030s, episode steps: 10, steps per second: 331, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.151 [-0.755, 1.533], loss: 0.405051, mean_absolute_error: 2.578325, mean_q: 5.021452\n",
            "  746/5000: episode: 43, duration: 0.039s, episode steps: 13, steps per second: 335, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.114 [-0.936, 1.450], loss: 0.733657, mean_absolute_error: 2.738381, mean_q: 5.085366\n",
            "  757/5000: episode: 44, duration: 0.033s, episode steps: 11, steps per second: 335, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.130 [-0.750, 1.421], loss: 0.535146, mean_absolute_error: 2.712759, mean_q: 5.132342\n",
            "  767/5000: episode: 45, duration: 0.030s, episode steps: 10, steps per second: 331, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.149 [-0.948, 1.717], loss: 0.688644, mean_absolute_error: 2.776280, mean_q: 5.251983\n",
            "  778/5000: episode: 46, duration: 0.034s, episode steps: 11, steps per second: 325, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.132 [-0.939, 1.526], loss: 0.406384, mean_absolute_error: 2.750758, mean_q: 5.290302\n",
            "  789/5000: episode: 47, duration: 0.040s, episode steps: 11, steps per second: 273, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.092 [-1.009, 1.481], loss: 0.618930, mean_absolute_error: 2.856010, mean_q: 5.404776\n",
            "  801/5000: episode: 48, duration: 0.036s, episode steps: 12, steps per second: 333, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.109 [-0.796, 1.371], loss: 0.639632, mean_absolute_error: 2.889621, mean_q: 5.442482\n",
            "  815/5000: episode: 49, duration: 0.044s, episode steps: 14, steps per second: 321, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.076 [-0.806, 1.252], loss: 0.635567, mean_absolute_error: 2.922610, mean_q: 5.503670\n",
            "  828/5000: episode: 50, duration: 0.039s, episode steps: 13, steps per second: 333, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-0.753, 1.323], loss: 0.923578, mean_absolute_error: 3.028144, mean_q: 5.603140\n",
            "  842/5000: episode: 51, duration: 0.042s, episode steps: 14, steps per second: 333, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.087 [-1.008, 1.435], loss: 0.875085, mean_absolute_error: 3.048500, mean_q: 5.593558\n",
            "  858/5000: episode: 52, duration: 0.050s, episode steps: 16, steps per second: 319, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.072 [-1.014, 1.404], loss: 0.823015, mean_absolute_error: 3.056484, mean_q: 5.680637\n",
            "  869/5000: episode: 53, duration: 0.033s, episode steps: 11, steps per second: 331, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.107 [-0.983, 1.515], loss: 0.758409, mean_absolute_error: 3.077687, mean_q: 5.809632\n",
            "  881/5000: episode: 54, duration: 0.037s, episode steps: 12, steps per second: 326, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.094 [-0.963, 1.642], loss: 0.761717, mean_absolute_error: 3.138265, mean_q: 5.949338\n",
            "  893/5000: episode: 55, duration: 0.038s, episode steps: 12, steps per second: 313, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.102 [-1.401, 2.201], loss: 0.690694, mean_absolute_error: 3.163744, mean_q: 6.006818\n",
            "  903/5000: episode: 56, duration: 0.031s, episode steps: 10, steps per second: 325, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.112 [-0.992, 1.549], loss: 0.789185, mean_absolute_error: 3.234103, mean_q: 6.097558\n",
            "  915/5000: episode: 57, duration: 0.036s, episode steps: 12, steps per second: 332, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.111 [-0.949, 1.523], loss: 0.595743, mean_absolute_error: 3.199587, mean_q: 6.123786\n",
            "  925/5000: episode: 58, duration: 0.034s, episode steps: 10, steps per second: 294, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.112 [-0.826, 1.399], loss: 0.672610, mean_absolute_error: 3.281940, mean_q: 6.209825\n",
            "  940/5000: episode: 59, duration: 0.044s, episode steps: 15, steps per second: 339, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.079 [-0.827, 1.286], loss: 0.856457, mean_absolute_error: 3.338307, mean_q: 6.254967\n",
            "  955/5000: episode: 60, duration: 0.044s, episode steps: 15, steps per second: 339, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.076 [-0.797, 1.241], loss: 0.779577, mean_absolute_error: 3.392685, mean_q: 6.377516\n",
            "  969/5000: episode: 61, duration: 0.044s, episode steps: 14, steps per second: 315, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.783, 1.373], loss: 0.879905, mean_absolute_error: 3.430099, mean_q: 6.414552\n",
            "  985/5000: episode: 62, duration: 0.058s, episode steps: 16, steps per second: 274, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.084 [-0.743, 1.206], loss: 0.785589, mean_absolute_error: 3.443666, mean_q: 6.416122\n",
            " 1001/5000: episode: 63, duration: 0.052s, episode steps: 16, steps per second: 305, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.100 [-0.754, 1.546], loss: 0.710682, mean_absolute_error: 3.480793, mean_q: 6.586400\n",
            " 1017/5000: episode: 64, duration: 0.047s, episode steps: 16, steps per second: 337, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.072 [-0.795, 1.170], loss: 0.909361, mean_absolute_error: 3.554914, mean_q: 6.743472\n",
            " 1032/5000: episode: 65, duration: 0.045s, episode steps: 15, steps per second: 333, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.094 [-0.767, 1.258], loss: 0.701493, mean_absolute_error: 3.576077, mean_q: 6.756502\n",
            " 1048/5000: episode: 66, duration: 0.049s, episode steps: 16, steps per second: 327, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.065 [-0.978, 1.407], loss: 0.777599, mean_absolute_error: 3.604901, mean_q: 6.841774\n",
            " 1059/5000: episode: 67, duration: 0.034s, episode steps: 11, steps per second: 328, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.113 [-0.765, 1.325], loss: 0.772747, mean_absolute_error: 3.650263, mean_q: 6.890056\n",
            " 1073/5000: episode: 68, duration: 0.048s, episode steps: 14, steps per second: 292, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.092 [-0.963, 1.473], loss: 1.385768, mean_absolute_error: 3.783267, mean_q: 6.869974\n",
            " 1087/5000: episode: 69, duration: 0.042s, episode steps: 14, steps per second: 334, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.114 [-0.779, 1.498], loss: 1.152293, mean_absolute_error: 3.766097, mean_q: 6.847072\n",
            " 1096/5000: episode: 70, duration: 0.028s, episode steps: 9, steps per second: 325, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-1.025, 1.605], loss: 1.124943, mean_absolute_error: 3.765660, mean_q: 6.916818\n",
            " 1105/5000: episode: 71, duration: 0.027s, episode steps: 9, steps per second: 329, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.143 [-0.955, 1.756], loss: 0.891215, mean_absolute_error: 3.744498, mean_q: 6.968898\n",
            " 1116/5000: episode: 72, duration: 0.033s, episode steps: 11, steps per second: 334, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.090 [-1.025, 1.595], loss: 0.915146, mean_absolute_error: 3.761610, mean_q: 7.006208\n",
            " 1127/5000: episode: 73, duration: 0.035s, episode steps: 11, steps per second: 314, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.132 [-0.756, 1.450], loss: 0.773389, mean_absolute_error: 3.807595, mean_q: 7.219706\n",
            " 1141/5000: episode: 74, duration: 0.047s, episode steps: 14, steps per second: 297, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.095 [-0.764, 1.277], loss: 1.215618, mean_absolute_error: 3.919738, mean_q: 7.246171\n",
            " 1154/5000: episode: 75, duration: 0.038s, episode steps: 13, steps per second: 339, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.128 [-0.934, 1.509], loss: 1.274766, mean_absolute_error: 3.934422, mean_q: 7.152017\n",
            " 1176/5000: episode: 76, duration: 0.063s, episode steps: 22, steps per second: 350, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.055 [-0.753, 1.066], loss: 1.060588, mean_absolute_error: 3.942301, mean_q: 7.210408\n",
            " 1189/5000: episode: 77, duration: 0.038s, episode steps: 13, steps per second: 340, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.119 [-0.549, 1.002], loss: 0.783836, mean_absolute_error: 3.920425, mean_q: 7.345774\n",
            " 1205/5000: episode: 78, duration: 0.047s, episode steps: 16, steps per second: 344, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.103 [-0.558, 1.069], loss: 1.288168, mean_absolute_error: 4.009046, mean_q: 7.353278\n",
            " 1222/5000: episode: 79, duration: 0.052s, episode steps: 17, steps per second: 326, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.093 [-0.549, 1.030], loss: 1.244745, mean_absolute_error: 4.060656, mean_q: 7.410386\n",
            " 1243/5000: episode: 80, duration: 0.061s, episode steps: 21, steps per second: 344, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.099 [-0.573, 0.915], loss: 1.138911, mean_absolute_error: 4.052509, mean_q: 7.438136\n",
            " 1267/5000: episode: 81, duration: 0.069s, episode steps: 24, steps per second: 349, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.543, 0.909], loss: 0.922769, mean_absolute_error: 4.059478, mean_q: 7.594468\n",
            " 1291/5000: episode: 82, duration: 0.068s, episode steps: 24, steps per second: 351, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.426, 0.890], loss: 1.123870, mean_absolute_error: 4.157656, mean_q: 7.698942\n",
            " 1313/5000: episode: 83, duration: 0.069s, episode steps: 22, steps per second: 319, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.544, 0.923], loss: 0.900399, mean_absolute_error: 4.163267, mean_q: 7.825586\n",
            " 1339/5000: episode: 84, duration: 0.074s, episode steps: 26, steps per second: 350, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.105 [-0.402, 0.855], loss: 1.029124, mean_absolute_error: 4.268315, mean_q: 8.038682\n",
            " 1369/5000: episode: 85, duration: 0.085s, episode steps: 30, steps per second: 351, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.099 [-0.381, 0.657], loss: 0.987147, mean_absolute_error: 4.301504, mean_q: 8.041826\n",
            " 1431/5000: episode: 86, duration: 0.181s, episode steps: 62, steps per second: 342, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.045 [-0.349, 0.702], loss: 1.080870, mean_absolute_error: 4.422875, mean_q: 8.314402\n",
            " 1459/5000: episode: 87, duration: 0.080s, episode steps: 28, steps per second: 350, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.622, 0.912], loss: 1.327195, mean_absolute_error: 4.532022, mean_q: 8.404446\n",
            " 1481/5000: episode: 88, duration: 0.065s, episode steps: 22, steps per second: 341, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.085 [-0.440, 1.165], loss: 1.261543, mean_absolute_error: 4.580336, mean_q: 8.515413\n",
            " 1504/5000: episode: 89, duration: 0.069s, episode steps: 23, steps per second: 334, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.080 [-0.544, 0.886], loss: 1.787067, mean_absolute_error: 4.660518, mean_q: 8.520361\n",
            " 1531/5000: episode: 90, duration: 0.083s, episode steps: 27, steps per second: 326, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.065 [-0.534, 0.776], loss: 1.252814, mean_absolute_error: 4.629911, mean_q: 8.593419\n",
            " 1567/5000: episode: 91, duration: 0.103s, episode steps: 36, steps per second: 350, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.092 [-0.608, 0.873], loss: 0.943964, mean_absolute_error: 4.674331, mean_q: 8.870894\n",
            " 1632/5000: episode: 92, duration: 0.188s, episode steps: 65, steps per second: 345, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.058 [-0.391, 0.720], loss: 1.277243, mean_absolute_error: 4.807197, mean_q: 9.002760\n",
            " 1705/5000: episode: 93, duration: 0.224s, episode steps: 73, steps per second: 326, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.062 [-0.258, 0.799], loss: 1.126977, mean_absolute_error: 4.929171, mean_q: 9.341529\n",
            " 1740/5000: episode: 94, duration: 0.106s, episode steps: 35, steps per second: 331, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.169 [-0.358, 0.780], loss: 1.131242, mean_absolute_error: 5.044332, mean_q: 9.569379\n",
            " 1835/5000: episode: 95, duration: 0.273s, episode steps: 95, steps per second: 348, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.159 [-0.521, 0.958], loss: 1.214986, mean_absolute_error: 5.218404, mean_q: 9.917546\n",
            " 1898/5000: episode: 96, duration: 0.190s, episode steps: 63, steps per second: 332, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.164 [-1.010, 0.287], loss: 1.318518, mean_absolute_error: 5.437569, mean_q: 10.349800\n",
            " 1989/5000: episode: 97, duration: 0.257s, episode steps: 91, steps per second: 354, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.223 [-0.315, 1.298], loss: 1.295711, mean_absolute_error: 5.629098, mean_q: 10.755543\n",
            " 2062/5000: episode: 98, duration: 0.217s, episode steps: 73, steps per second: 337, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.022 [-0.998, 0.621], loss: 1.302773, mean_absolute_error: 5.846776, mean_q: 11.247413\n",
            " 2078/5000: episode: 99, duration: 0.047s, episode steps: 16, steps per second: 339, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-0.973, 0.547], loss: 1.371680, mean_absolute_error: 5.941505, mean_q: 11.598343\n",
            " 2096/5000: episode: 100, duration: 0.053s, episode steps: 18, steps per second: 340, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-0.933, 0.412], loss: 1.580785, mean_absolute_error: 6.104464, mean_q: 11.857821\n",
            " 2115/5000: episode: 101, duration: 0.060s, episode steps: 19, steps per second: 315, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.111 [-1.004, 0.387], loss: 1.465742, mean_absolute_error: 6.151679, mean_q: 11.975136\n",
            " 2137/5000: episode: 102, duration: 0.072s, episode steps: 22, steps per second: 304, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-0.821, 0.402], loss: 1.614773, mean_absolute_error: 6.122360, mean_q: 11.784369\n",
            " 2178/5000: episode: 103, duration: 0.118s, episode steps: 41, steps per second: 347, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.064 [-0.762, 0.392], loss: 1.548128, mean_absolute_error: 6.248705, mean_q: 12.088008\n",
            " 2202/5000: episode: 104, duration: 0.076s, episode steps: 24, steps per second: 318, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.849, 0.379], loss: 1.321354, mean_absolute_error: 6.350028, mean_q: 12.337539\n",
            " 2218/5000: episode: 105, duration: 0.055s, episode steps: 16, steps per second: 292, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.901, 0.579], loss: 2.326708, mean_absolute_error: 6.451193, mean_q: 12.343573\n",
            " 2241/5000: episode: 106, duration: 0.067s, episode steps: 23, steps per second: 343, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.083 [-0.808, 0.409], loss: 1.773342, mean_absolute_error: 6.517611, mean_q: 12.568424\n",
            " 2268/5000: episode: 107, duration: 0.086s, episode steps: 27, steps per second: 313, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.067 [-0.741, 0.433], loss: 1.182128, mean_absolute_error: 6.564771, mean_q: 12.808030\n",
            " 2309/5000: episode: 108, duration: 0.125s, episode steps: 41, steps per second: 329, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.049 [-0.810, 0.425], loss: 2.050793, mean_absolute_error: 6.699822, mean_q: 12.929075\n",
            " 2334/5000: episode: 109, duration: 0.072s, episode steps: 25, steps per second: 345, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.088 [-1.142, 0.417], loss: 0.738130, mean_absolute_error: 6.697747, mean_q: 13.214838\n",
            " 2352/5000: episode: 110, duration: 0.068s, episode steps: 18, steps per second: 265, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.099 [-1.068, 0.581], loss: 2.775507, mean_absolute_error: 6.936399, mean_q: 13.356825\n",
            " 2365/5000: episode: 111, duration: 0.043s, episode steps: 13, steps per second: 305, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.106 [-1.297, 0.622], loss: 2.302530, mean_absolute_error: 6.923491, mean_q: 13.238181\n",
            " 2381/5000: episode: 112, duration: 0.051s, episode steps: 16, steps per second: 313, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.077 [-1.225, 0.807], loss: 1.738529, mean_absolute_error: 7.087105, mean_q: 13.696669\n",
            " 2400/5000: episode: 113, duration: 0.056s, episode steps: 19, steps per second: 337, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.092 [-1.048, 0.559], loss: 2.667016, mean_absolute_error: 7.070189, mean_q: 13.477248\n",
            " 2427/5000: episode: 114, duration: 0.082s, episode steps: 27, steps per second: 328, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.087 [-1.166, 0.380], loss: 2.691878, mean_absolute_error: 7.065052, mean_q: 13.397571\n",
            " 2454/5000: episode: 115, duration: 0.083s, episode steps: 27, steps per second: 325, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.057 [-1.036, 0.382], loss: 2.551853, mean_absolute_error: 7.132320, mean_q: 13.697803\n",
            " 2468/5000: episode: 116, duration: 0.041s, episode steps: 14, steps per second: 339, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.116 [-1.341, 0.612], loss: 2.240208, mean_absolute_error: 7.230072, mean_q: 13.952867\n",
            " 2489/5000: episode: 117, duration: 0.061s, episode steps: 21, steps per second: 344, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.091 [-1.064, 0.580], loss: 1.673167, mean_absolute_error: 7.268739, mean_q: 14.119274\n",
            " 2501/5000: episode: 118, duration: 0.036s, episode steps: 12, steps per second: 333, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.128 [-1.256, 0.610], loss: 2.412115, mean_absolute_error: 7.319546, mean_q: 14.105145\n",
            " 2509/5000: episode: 119, duration: 0.025s, episode steps: 8, steps per second: 323, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.135 [-1.994, 1.223], loss: 1.498000, mean_absolute_error: 7.178432, mean_q: 13.921381\n",
            " 2518/5000: episode: 120, duration: 0.027s, episode steps: 9, steps per second: 328, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.848, 1.726], loss: 1.874881, mean_absolute_error: 7.237831, mean_q: 14.112186\n",
            " 2536/5000: episode: 121, duration: 0.056s, episode steps: 18, steps per second: 319, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.071 [-1.195, 0.586], loss: 2.203895, mean_absolute_error: 7.302994, mean_q: 14.204475\n",
            " 2544/5000: episode: 122, duration: 0.025s, episode steps: 8, steps per second: 324, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.528, 1.529], loss: 2.556658, mean_absolute_error: 7.492949, mean_q: 14.520385\n",
            " 2557/5000: episode: 123, duration: 0.039s, episode steps: 13, steps per second: 337, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.923 [0.000, 1.000], mean observation: -0.115 [-3.287, 2.148], loss: 4.959586, mean_absolute_error: 7.520475, mean_q: 14.285135\n",
            " 2568/5000: episode: 124, duration: 0.037s, episode steps: 11, steps per second: 297, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.126 [-2.827, 1.758], loss: 1.581685, mean_absolute_error: 7.418718, mean_q: 14.354090\n",
            " 2576/5000: episode: 125, duration: 0.025s, episode steps: 8, steps per second: 321, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.589, 1.596], loss: 3.482254, mean_absolute_error: 7.553205, mean_q: 14.441078\n",
            " 2584/5000: episode: 126, duration: 0.025s, episode steps: 8, steps per second: 317, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.569, 1.602], loss: 2.607283, mean_absolute_error: 7.414435, mean_q: 14.304543\n",
            " 2597/5000: episode: 127, duration: 0.039s, episode steps: 13, steps per second: 338, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.103 [-2.814, 1.808], loss: 4.293344, mean_absolute_error: 7.643056, mean_q: 14.482427\n",
            " 2619/5000: episode: 128, duration: 0.070s, episode steps: 22, steps per second: 312, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.079 [-1.071, 0.567], loss: 3.119144, mean_absolute_error: 7.659066, mean_q: 14.475862\n",
            " 2638/5000: episode: 129, duration: 0.056s, episode steps: 19, steps per second: 342, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.075 [-1.313, 0.583], loss: 3.535104, mean_absolute_error: 7.656000, mean_q: 14.476516\n",
            " 2654/5000: episode: 130, duration: 0.047s, episode steps: 16, steps per second: 339, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.106 [-1.028, 0.571], loss: 3.556196, mean_absolute_error: 7.711185, mean_q: 14.584713\n",
            " 2677/5000: episode: 131, duration: 0.071s, episode steps: 23, steps per second: 326, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.051 [-1.013, 0.424], loss: 2.911166, mean_absolute_error: 7.638348, mean_q: 14.517402\n",
            " 2701/5000: episode: 132, duration: 0.074s, episode steps: 24, steps per second: 323, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.057 [-1.075, 0.442], loss: 2.648753, mean_absolute_error: 7.728088, mean_q: 14.802078\n",
            " 2720/5000: episode: 133, duration: 0.055s, episode steps: 19, steps per second: 343, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.089 [-1.056, 0.433], loss: 2.921807, mean_absolute_error: 7.744166, mean_q: 14.825057\n",
            " 2736/5000: episode: 134, duration: 0.048s, episode steps: 16, steps per second: 336, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.109 [-1.142, 0.398], loss: 4.066757, mean_absolute_error: 8.013697, mean_q: 15.151947\n",
            " 2758/5000: episode: 135, duration: 0.065s, episode steps: 22, steps per second: 337, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-0.923, 0.545], loss: 3.893213, mean_absolute_error: 7.883686, mean_q: 14.929175\n",
            " 2775/5000: episode: 136, duration: 0.054s, episode steps: 17, steps per second: 315, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.099 [-1.100, 0.598], loss: 2.959657, mean_absolute_error: 7.770170, mean_q: 14.812593\n",
            " 2796/5000: episode: 137, duration: 0.061s, episode steps: 21, steps per second: 347, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.087 [-0.923, 0.421], loss: 2.943073, mean_absolute_error: 7.952134, mean_q: 15.127716\n",
            " 2831/5000: episode: 138, duration: 0.101s, episode steps: 35, steps per second: 347, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.093 [-0.737, 0.346], loss: 3.142108, mean_absolute_error: 8.011272, mean_q: 15.277042\n",
            " 2872/5000: episode: 139, duration: 0.125s, episode steps: 41, steps per second: 328, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.115 [-0.996, 0.287], loss: 3.601292, mean_absolute_error: 7.948684, mean_q: 15.102348\n",
            " 2915/5000: episode: 140, duration: 0.121s, episode steps: 43, steps per second: 356, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.088 [-0.936, 0.371], loss: 3.494996, mean_absolute_error: 8.107471, mean_q: 15.460448\n",
            " 2940/5000: episode: 141, duration: 0.073s, episode steps: 25, steps per second: 344, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.120 [-0.846, 0.193], loss: 3.523267, mean_absolute_error: 8.126368, mean_q: 15.465943\n",
            " 2996/5000: episode: 142, duration: 0.161s, episode steps: 56, steps per second: 349, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-1.126, 0.434], loss: 4.007567, mean_absolute_error: 8.269218, mean_q: 15.740080\n",
            " 3050/5000: episode: 143, duration: 0.181s, episode steps: 54, steps per second: 299, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.093 [-0.851, 0.263], loss: 2.902705, mean_absolute_error: 8.307672, mean_q: 15.956531\n",
            " 3125/5000: episode: 144, duration: 0.216s, episode steps: 75, steps per second: 347, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.114 [-0.260, 1.114], loss: 3.901839, mean_absolute_error: 8.449553, mean_q: 16.074812\n",
            " 3163/5000: episode: 145, duration: 0.111s, episode steps: 38, steps per second: 341, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.147 [-0.759, 0.364], loss: 2.659459, mean_absolute_error: 8.486802, mean_q: 16.373791\n",
            " 3207/5000: episode: 146, duration: 0.129s, episode steps: 44, steps per second: 342, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.118 [-0.730, 0.268], loss: 4.115735, mean_absolute_error: 8.646300, mean_q: 16.557619\n",
            " 3285/5000: episode: 147, duration: 0.229s, episode steps: 78, steps per second: 340, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.071 [-1.025, 0.316], loss: 3.294314, mean_absolute_error: 8.684294, mean_q: 16.658424\n",
            " 3346/5000: episode: 148, duration: 0.172s, episode steps: 61, steps per second: 354, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.118 [-0.780, 0.276], loss: 3.650461, mean_absolute_error: 8.826682, mean_q: 16.924864\n",
            " 3408/5000: episode: 149, duration: 0.182s, episode steps: 62, steps per second: 341, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.089 [-0.419, 0.977], loss: 3.172504, mean_absolute_error: 8.811257, mean_q: 16.977057\n",
            " 3461/5000: episode: 150, duration: 0.155s, episode steps: 53, steps per second: 341, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.105 [-0.409, 1.033], loss: 3.224643, mean_absolute_error: 9.012788, mean_q: 17.414410\n",
            " 3525/5000: episode: 151, duration: 0.190s, episode steps: 64, steps per second: 337, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.103 [-0.755, 0.342], loss: 3.823446, mean_absolute_error: 9.118933, mean_q: 17.495285\n",
            " 3661/5000: episode: 152, duration: 0.395s, episode steps: 136, steps per second: 345, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.058 [-0.566, 0.914], loss: 3.680466, mean_absolute_error: 9.232493, mean_q: 17.808733\n",
            " 3719/5000: episode: 153, duration: 0.168s, episode steps: 58, steps per second: 345, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.146 [-0.774, 0.476], loss: 4.022723, mean_absolute_error: 9.419900, mean_q: 18.146358\n",
            " 3772/5000: episode: 154, duration: 0.160s, episode steps: 53, steps per second: 331, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.138 [-0.201, 0.735], loss: 4.472262, mean_absolute_error: 9.518994, mean_q: 18.243734\n",
            " 3869/5000: episode: 155, duration: 0.280s, episode steps: 97, steps per second: 347, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.140 [-0.793, 0.340], loss: 4.029566, mean_absolute_error: 9.503116, mean_q: 18.222948\n",
            " 3929/5000: episode: 156, duration: 0.168s, episode steps: 60, steps per second: 356, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.143 [-0.369, 0.882], loss: 3.156132, mean_absolute_error: 9.633138, mean_q: 18.754297\n",
            " 4000/5000: episode: 157, duration: 0.204s, episode steps: 71, steps per second: 348, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.125 [-0.385, 0.911], loss: 3.412526, mean_absolute_error: 9.802429, mean_q: 18.985727\n",
            " 4053/5000: episode: 158, duration: 0.150s, episode steps: 53, steps per second: 352, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.162 [-0.263, 0.915], loss: 3.801827, mean_absolute_error: 9.851114, mean_q: 19.149290\n",
            " 4183/5000: episode: 159, duration: 0.383s, episode steps: 130, steps per second: 340, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.095 [-0.673, 0.472], loss: 3.844810, mean_absolute_error: 9.946272, mean_q: 19.303825\n",
            " 4241/5000: episode: 160, duration: 0.165s, episode steps: 58, steps per second: 351, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.189 [-0.215, 1.078], loss: 2.904916, mean_absolute_error: 10.077236, mean_q: 19.646971\n",
            " 4351/5000: episode: 161, duration: 0.315s, episode steps: 110, steps per second: 349, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.153 [-0.626, 0.779], loss: 3.927934, mean_absolute_error: 10.318671, mean_q: 20.101648\n",
            " 4440/5000: episode: 162, duration: 0.259s, episode steps: 89, steps per second: 344, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.127 [-0.236, 1.061], loss: 4.264904, mean_absolute_error: 10.550706, mean_q: 20.394470\n",
            " 4490/5000: episode: 163, duration: 0.142s, episode steps: 50, steps per second: 352, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.199 [-0.369, 1.121], loss: 4.447934, mean_absolute_error: 10.649433, mean_q: 20.562820\n",
            " 4608/5000: episode: 164, duration: 0.350s, episode steps: 118, steps per second: 337, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.128 [-0.710, 1.061], loss: 3.510949, mean_absolute_error: 10.729850, mean_q: 20.954792\n",
            " 4702/5000: episode: 165, duration: 0.272s, episode steps: 94, steps per second: 346, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.157 [-0.273, 1.232], loss: 4.095547, mean_absolute_error: 10.970571, mean_q: 21.353146\n",
            " 4885/5000: episode: 166, duration: 0.526s, episode steps: 183, steps per second: 348, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.102 [-0.672, 1.097], loss: 3.596015, mean_absolute_error: 11.208464, mean_q: 21.874893\n",
            " 4992/5000: episode: 167, duration: 0.308s, episode steps: 107, steps per second: 348, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.187 [-0.389, 1.309], loss: 3.983833, mean_absolute_error: 11.468054, mean_q: 22.374578\n",
            "done, took 15.449 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5f6a31908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_n-QkqZprhA",
        "colab_type": "text"
      },
      "source": [
        "Test our reinforcement learning model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1Lk_ZgmxI1",
        "colab_type": "code",
        "outputId": "1aa6b403-c3be-45fb-b839-89778b814b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "dqn.test(env, nb_episodes=5, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 67.000, steps: 67\n",
            "Episode 2: reward: 82.000, steps: 82\n",
            "Episode 3: reward: 57.000, steps: 57\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 130.000, steps: 130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa61e275c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3U_MOLGpstr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}