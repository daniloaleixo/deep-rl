{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DataLoader():\n",
    "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
    "\n",
    "    def __init__(self, filename, split, cols):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "        \n",
    "        i_split = int(len(dataframe) * split)\n",
    "        self.data_train = dataframe.get(cols).values[:i_split]\n",
    "        self.data_test  = dataframe.get(cols).values[i_split:]\n",
    "        self.len_train  = len(self.data_train)\n",
    "        self.len_test   = len(self.data_test)\n",
    "        self.len_train_windows = None\n",
    "\n",
    "    def get_test_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y test data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise reduce size of the training split.\n",
    "        '''\n",
    "        data_windows = []\n",
    "        for i in range(self.len_test - seq_len):\n",
    "            data_windows.append(self.data_test[i:i+seq_len])\n",
    "\n",
    "        data_windows = np.array(data_windows).astype(float)\n",
    "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
    "\n",
    "        x = data_windows[:, :-1]\n",
    "        y = data_windows[:, -1, [0]]\n",
    "        return x,y\n",
    "\n",
    "    def get_train_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y train data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise use generate_training_window() method.\n",
    "        '''\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(self.len_train - seq_len):\n",
    "            x, y = self._next_window(i, seq_len, normalise)\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "        return np.array(data_x), np.array(data_y)\n",
    "\n",
    "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
    "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
    "        i = 0\n",
    "        while i < (self.len_train - seq_len):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for b in range(batch_size):\n",
    "                if i >= (self.len_train - seq_len):\n",
    "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
    "                    yield np.array(x_batch), np.array(y_batch)\n",
    "                    i = 0\n",
    "                x, y = self._next_window(i, seq_len, normalise)\n",
    "                x_batch.append(x)\n",
    "                y_batch.append(y)\n",
    "                i += 1\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "    def _next_window(self, i, seq_len, normalise):\n",
    "        '''Generates the next data window from the given index location i'''\n",
    "        window = self.data_train[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:-1]\n",
    "        y = window[-1, [0]]\n",
    "        return x, y\n",
    "\n",
    "    def normalise_windows(self, window_data, single_window=False):\n",
    "        '''Normalise window with a base value of zero'''\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                normalised_col = [((float(p) / (float(window[0, col_i]) + 0.00000001) ) - 1) for p in window[:, col_i]]\n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "configs = json.load(open('config.json', 'r'))\n",
    "if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "pre_trained_model_filepath = configs['model']['pre_trained']\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    os.path.join('data', configs['data']['filename']),\n",
    "    configs['data']['train_test_split'],\n",
    "    configs['data']['columns']\n",
    ")\n",
    "x_train, y_train = dataloader.get_train_data(\n",
    "        seq_len=configs['data']['sequence_length'],\n",
    "        normalise=configs['data']['normalise']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 29, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = configs['data']['columns']\n",
    "seq_len = configs['data']['sequence_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "# Here our possible actions\n",
    "sit = [1, 0, 0]\n",
    "buy = [0, 1, 0]\n",
    "sell = [0, 0, 1]\n",
    "possible_actions = [sit, buy, sell]\n",
    "state_size = (seq_len -1, len(columns))      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = 3              # 3 possible actions: sit, buy, sell\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 10       # Total episodes for training\n",
    "max_steps = 1000              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, learning_rate, gamma=0.95, \n",
    "                 explore_start=1.0, explore_stop=0.01, decay_rate=0.00007, possible_actions = [], name='Agent', \n",
    "                 is_eval=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.explore_start = explore_start\n",
    "        self.explore_stop = explore_stop\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_step = 0\n",
    "        self.explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * self.decay_step)\n",
    "        \n",
    "        self.possible_actions = possible_actions\n",
    "        \n",
    "        self.model = load_model(\"models/\" + model_name) if is_eval else self._model()\n",
    "        \n",
    "        self.loss = 9999\n",
    "        \n",
    "    def _model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, input_shape=state_size, activation=\"relu\"))\n",
    "        model.add(Dense(units=32, activation=\"relu\"))\n",
    "        model.add(Dense(units=8, activation=\"relu\"))\n",
    "        model.add(Flatten())\n",
    "#         model.add(Dense(units=32, activation=\"relu\"))\n",
    "#         model.add(Dense(units=8, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"softmax\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def predict_point_by_point(self, data):\n",
    "        predicted = self.model.predict(data)\n",
    "        predicted = np.reshape(predicted, (predicted.size,))\n",
    "        return predicted\n",
    "    \n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "            \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "                \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            hist = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            self.loss = hist.history['loss'].pop()\n",
    "            \n",
    "        if self.explore_probability > self.explore_stop:\n",
    "            self.explore_probability = self.explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "            \n",
    "    def predict_action(self, state):\n",
    "        ## EPSILON GREEDY STRATEGY\n",
    "        # Choose action a from state s using epsilon greedy.\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "        # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "        self.explore_probability = self.explore_stop + (self.explore_start - self.explore_stop) * np.exp(-self.decay_rate * self.decay_step)\n",
    "\n",
    "#         print(\"comparar \", self.explore_probability > exp_exp_tradeoff, self.explore_probability, exp_exp_tradeoff)\n",
    "        if (self.explore_probability > exp_exp_tradeoff):\n",
    "            # Make a random action (exploration)\n",
    "            action = np.argmax(random.choice(self.possible_actions))\n",
    "    #         print(\"random \", action)\n",
    "        else:\n",
    "            predicted = self.predict_point_by_point(state)\n",
    "    #         print(\"array predicted \", predicted)\n",
    "            action = np.argmax(predicted)\n",
    "    #         print(\"predicted \", action)\n",
    "\n",
    "        self.decay_step += 1\n",
    "\n",
    "        return action, self.explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 29, 64)            128       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 29, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 29, 8)             264       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 232)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 699       \n",
      "=================================================================\n",
      "Total params: 3,171\n",
      "Trainable params: 3,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DQNetwork = Agent(state_size, action_size, learning_rate, gamma, possible_actions=possible_actions, decay_rate=decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x_train[0:200]\n",
    "def next_state(index=0):\n",
    "    return data[index:index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decay rate (that will use to reduce epsilon) \n",
    "decay_step = 0\n",
    "\n",
    "#  Set step to 0\n",
    "step = 0\n",
    "\n",
    "cash = 10_000\n",
    "first_price = dataloader.data_train[0][0]\n",
    "\n",
    "total_profit = 0\n",
    "position = 0\n",
    "last_transaction = 0\n",
    "\n",
    "# Initialize the rewards of the episode\n",
    "episode_rewards = []\n",
    "rewards = []\n",
    "\n",
    "debug = False\n",
    "\n",
    "state = next_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_action(state, action, last_transaction=0, total_profit=0, \n",
    "                position=0, first_price=0):\n",
    "    close = state[0][-1][0]\n",
    "    not_do_anything_reward = 0\n",
    "    if(action == 0): \n",
    "        return not_do_anything_reward, last_transaction, total_profit, position\n",
    "    if (action == 1 and position == 0):\n",
    "        position = 1\n",
    "        last_transaction = (1.0 + close) * first_price\n",
    "        if debug: print(\"Buy: $\", '%.2f' % last_transaction)\n",
    "        return not_do_anything_reward, last_transaction, total_profit, position\n",
    "    if(action == 2 and position == 1):\n",
    "        position = 0\n",
    "        price_now = (1.0 + close) * first_price\n",
    "        profit = price_now - last_transaction\n",
    "        profit_per = (price_now / last_transaction - 1.0) * 100\n",
    "        total_profit += profit\n",
    "        if debug: print(\"Sell: $\", '%.2f' % price_now, \n",
    "                        \" - Profit $\", '%.2f' % profit, \n",
    "                        \" - \", '%.2f' % profit_per, \"%\" )\n",
    "        return profit, last_transaction, total_profit, position\n",
    "    return not_do_anything_reward, last_transaction, total_profit, position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode # 1\n",
      "Episode: 1 Total reward: -179.67799890924562 Training loss: 0.0716 Explore P: 0.8461\n",
      "Episode # 2\n",
      "Episode: 2 Total reward: -175.26321342498431 Training loss: 0.0111 Explore P: 0.7154\n",
      "Episode # 3\n",
      "Episode: 3 Total reward: 143.97706143051687 Training loss: 0.0947 Explore P: 0.6051\n",
      "Episode # 4\n",
      "Episode: 4 Total reward: 60.3103119937598 Training loss: 1672.3691 Explore P: 0.5121\n",
      "Episode # 5\n",
      "Episode: 5 Total reward: 90.79908547024502 Training loss: 0.2742 Explore P: 0.4336\n",
      "Episode # 6\n",
      "Episode: 6 Total reward: 71.39233839427447 Training loss: 431.2334 Explore P: 0.3673\n",
      "Episode # 7\n",
      "Episode: 7 Total reward: 273.4584437634039 Training loss: 0.1256 Explore P: 0.3115\n",
      "Episode # 8\n",
      "Episode: 8 Total reward: 112.46551121952507 Training loss: 0.0058 Explore P: 0.2643\n",
      "Episode # 9\n",
      "Episode: 9 Total reward: 169.97833266861676 Training loss: 0.1225 Explore P: 0.2246\n",
      "Episode # 10\n",
      "Episode: 10 Total reward: 115.9514152814861 Training loss: 0.0412 Explore P: 0.1910\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, total_episodes):\n",
    "    print(\"Episode #\", i + 1)\n",
    "    step = 0\n",
    "    episode_rewards = []\n",
    "    my_reward = 0\n",
    "\n",
    "    while step < len(data) - seq_len:\n",
    "        step += 1\n",
    "#         if debug: print(\"Step: \", step)\n",
    "\n",
    "        # Increase decay_step\n",
    "        decay_step +=1\n",
    "\n",
    "        # Predict the action to take and take it\n",
    "        action, explore_probability = DQNetwork.predict_action(state)\n",
    "\n",
    "        # Do the action\n",
    "        reward, last_transaction, total_profit, position = make_action(\n",
    "            state, action, last_transaction, total_profit, position, first_price)\n",
    "\n",
    "        # Add the reward to total reward of the episode\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        # Add to memory \n",
    "        next_st = next_state(step)\n",
    "        done = True if step >= len(data) - seq_len else False\n",
    "        DQNetwork.memory.append((state, action, reward, next_st, done))\n",
    "        \n",
    "        state = next_st\n",
    "        \n",
    "        my_reward +=reward\n",
    "        \n",
    "        print(\"Step \", step, \" of \", len(data) - seq_len,\n",
    "               \" - Loss:\", '%.4f' % DQNetwork.loss, \" - Reward \", '%.4f' %  my_reward, end=\"\\r\")\n",
    "        \n",
    "        # Learning         \n",
    "        if len(DQNetwork.memory) > batch_size:\n",
    "            DQNetwork.expReplay(batch_size)\n",
    "        \n",
    "\n",
    "        # If done\n",
    "        if done:            \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            rewards.append(total_reward)\n",
    "            DQNetwork.model.save(\"saved_models/model_ep\" + str(i))\n",
    "\n",
    "            print('Episode: {}'.format(i + 1),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(DQNetwork.loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = dataloader.get_test_data(\n",
    "\tseq_len = configs['data']['sequence_length'],\n",
    "\tnormalise = configs['data']['normalise']\n",
    ")\n",
    "data = x_test[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1\n",
      "Step  1  of  70  - Loss: 0.0412  - Reward  0.0000\r",
      "Step:  2\n",
      "Step  2  of  70  - Loss: 0.0412  - Reward  0.0000\r",
      "Step:  3\n",
      "Sell: $ 1482.70  - Profit $ 40.77  -  2.83 %\n",
      "Step  3  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  4\n",
      "Step  4  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  5\n",
      "Step  5  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  6\n",
      "Step  6  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  7\n",
      "Step  7  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  8\n",
      "Step  8  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  9\n",
      "Step  9  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  10\n",
      "Step  10  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  11\n",
      "Step  11  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  12\n",
      "Step  12  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  13\n",
      "Step  13  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  14\n",
      "Step  14  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  15\n",
      "Step  15  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  16\n",
      "Step  16  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  17\n",
      "Step  17  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  18\n",
      "Step  18  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  19\n",
      "Step  19  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  20\n",
      "Step  20  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  21\n",
      "Step  21  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  22\n",
      "Buy: $ 1493.19\n",
      "Step  22  of  70  - Loss: 0.0412  - Reward  40.7662\r",
      "Step:  23\n",
      "Sell: $ 1494.71  - Profit $ 1.52  -  0.10 %\n",
      "Step  23  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  24\n",
      "Step  24  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  25\n",
      "Step  25  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  26\n",
      "Step  26  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  27\n",
      "Step  27  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  28\n",
      "Step  28  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  29\n",
      "Step  29  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  30\n",
      "Step  30  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  31\n",
      "Step  31  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  32\n",
      "Step  32  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  33\n",
      "Step  33  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  34\n",
      "Step  34  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  35\n",
      "Step  35  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  36\n",
      "Step  36  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  37\n",
      "Step  37  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  38\n",
      "Buy: $ 1472.29\n",
      "Step  38  of  70  - Loss: 0.0412  - Reward  42.2857\r",
      "Step:  39\n",
      "Sell: $ 1483.25  - Profit $ 10.96  -  0.74 %\n",
      "Step  39  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  40\n",
      "Step  40  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  41\n",
      "Step  41  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  42\n",
      "Step  42  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  43\n",
      "Step  43  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  44\n",
      "Step  44  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  45\n",
      "Step  45  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  46\n",
      "Step  46  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  47\n",
      "Step  47  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  48\n",
      "Step  48  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  49\n",
      "Step  49  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  50\n",
      "Step  50  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  51\n",
      "Step  51  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  52\n",
      "Step  52  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  53\n",
      "Step  53  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  54\n",
      "Step  54  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  55\n",
      "Step  55  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  56\n",
      "Step  56  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  57\n",
      "Step  57  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  58\n",
      "Step  58  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  59\n",
      "Step  59  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  60\n",
      "Step  60  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  61\n",
      "Step  61  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  62\n",
      "Step  62  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  63\n",
      "Step  63  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  64\n",
      "Step  64  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  65\n",
      "Step  65  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  66\n",
      "Step  66  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  67\n",
      "Step  67  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  68\n",
      "Step  68  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  69\n",
      "Step  69  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Step:  70\n",
      "Step  70  of  70  - Loss: 0.0412  - Reward  53.2427\r",
      "Episode: 10 Total reward: 53.24274174132029 Training loss: 0.0412 Explore P: 0.1788\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "episode_rewards = []\n",
    "my_reward = 0\n",
    "\n",
    "while step < len(data) - seq_len:\n",
    "    step += 1\n",
    "    if debug: print(\"Step: \", step)\n",
    "\n",
    "    # Predict the action to take and take it\n",
    "    action, explore_probability = DQNetwork.predict_action(state)\n",
    "\n",
    "    # Do the action\n",
    "    reward, last_transaction, total_profit, position = make_action(\n",
    "        state, action, last_transaction, total_profit, position, first_price)\n",
    "\n",
    "    # Add the reward to total reward of the episode\n",
    "    episode_rewards.append(reward)\n",
    "\n",
    "    # Add to memory \n",
    "    next_st = next_state(step)\n",
    "    done = True if step >= len(data) - seq_len else False\n",
    "    DQNetwork.memory.append((state, action, reward, next_st, done))\n",
    "\n",
    "    state = next_st\n",
    "\n",
    "    my_reward +=reward\n",
    "\n",
    "    print(\"Step \", step, \" of \", len(data) - seq_len,\n",
    "           \" - Loss:\", '%.4f' % DQNetwork.loss, \" - Reward \", '%.4f' %  my_reward, end=\"\\r\")\n",
    "\n",
    "    # Learning         \n",
    "#     if len(DQNetwork.memory) > batch_size:\n",
    "#         DQNetwork.expReplay(batch_size)\n",
    "\n",
    "\n",
    "    # If done\n",
    "    if done:            \n",
    "        total_reward = np.sum(episode_rewards)\n",
    "        rewards.append(total_reward)\n",
    "        DQNetwork.model.save(\"saved_models/model_ep\" + str(i))\n",
    "\n",
    "        print('Episode: {}'.format(i + 1),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(DQNetwork.loss),\n",
    "                  'Explore P: {:.4f}'.format(explore_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
